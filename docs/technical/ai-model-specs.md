# AI Model Specifications

## 1. 概述

本文件詳細描述了 `Codex-Scribe` 專案中使用的 AI 模型（大型語言模型和嵌入模型）的選擇、配置和使用規範。

**目標讀者**: 開發者、AI 工程師。

## 2. 模型選擇

本專案採用混合模型的策略，針對不同任務選擇最合適的模型。

### 2.1 大型語言模型 (LLMs)

- **主要模型 (用於複雜推理和生成)**:
    - **`OpenAI GPT-4o`**: 作為預設的主要模型。它在程式碼理解、遵循指令和結構化輸出方面表現出色，綜合能力強。
- **備選/專用模型**:
    - **`Anthropic Claude 3.5 Sonnet`**: 作為 GPT-4o 的高性能、高性價比替代品。在需要快速響應或處理長篇程式碼的場景下可以考慮使用。
    - **`Google Gemini 1.5 Pro`**: 當需要處理超長上下文（高達 1M token）時，例如一次性分析整個大型文件或多個文件，此模型是首選。

### 2.2 嵌入模型 (Embedding Models)

- **主要模型**:
    - **`OpenAI text-embedding-3-large`**: 作為預設的嵌入模型。它性能優異，能夠很好地捕捉程式碼和自然語言的語義。
- **備選/專用模型**:
    - **`Voyage AI voyage-code-2`**: 專為程式碼優化的嵌入模型。在後續的性能評估中，如果發現程式碼語義搜索的準確性有待提高，可以考慮切換或混合使用此模型。

## 3. 模型配置與使用

### 3.1 LLM 客戶端 (`src/infrastructure/llm/llm_client.py`)

- **抽象化**: 應建立一個 `LLMClient` 的抽象基類 (ABC)，並為每個模型提供方（如 `OpenAIClient`, `AnthropicClient`）提供具體的實現。這使得在上層應用中可以輕鬆切換模型。
- **統一接口**: 所有客戶端都應提供一個統一的 `generate` 方法，接收標準化的輸入（如提示、停止序列）並返回標準化的輸出。

### 3.2 LLM 呼叫參數

- **`temperature`**:
    - 對於需要**創造性**和**生成性**的任務（如撰寫問題報告的描述），可以設置為 `0.3` - `0.5`。
    - 對於需要**確定性**和**事實性**的任務（如從文本中提取實體、格式化 JSON），應設置為 `0.0` - `0.1`。
- **`max_tokens`**: 應根據任務類型合理設置，以避免不必要的 token 浪費和成本。
- **`top_p`**: 通常保持預設值 (`1.0`)，除非需要更精細地控制輸出的隨機性。
- **JSON Mode**: 盡可能使用模型提供方支持的 JSON 模式，以確保輸出的格式正確性。

## 4. 智能路由與檢索策略

為了提供更精準、高效的回答，系統採用了智能路由和動態檢索策略。

### 4.1 混合式查詢分類器 (`_classify_query_intent`)

- **目的**: 準確判斷用戶查詢的意圖，以選擇最合適的檢索工具（向量搜索或圖形查詢）。
- **主要方法**:
    - **LLM 分類器**: 使用 `GPT-4o` 作為主要分類器。Prompt 中包含多語言（英文、繁體中文）的 few-shot 範例，引導模型輸出包含 `type`, `entity`, `confidence` 的 JSON 物件。
- **後備機制 (Fallback)**:
    - 當 LLM 回傳的信心度低於 `0.75`，或因任何原因（如 API 錯誤、回傳格式不正確）導致分類失敗時，系統會自動後備至基於正則表達式的分類器。
    - 這個後備機制確保了即使在 LLM 不穩定的情況下，系統依然能夠處理明確的查詢，保證了系統的穩定性和可靠性。

### 4.2 動態 Top-K 檢索 (`_get_optimal_top_k`)

- **目的**: 根據查詢的複雜度動態調整檢索的文檔數量 (`top_k`)，以平衡回答的全面性和檢索效率。
- **複雜度判斷**:
    - 系統會根據一系列啟發式規則（如查詢長度、是否包含 "如何"、"為什麼" 等複雜關鍵詞、問題數量）來判斷查詢的複雜度。
- **動態調整**:
    - **簡單查詢**: `top_k` 設置為 `5`，以快速提供精準答案。
    - **複雜查詢**: `top_k` 設置為 `10`，以獲取更廣泛的上下文，幫助 LLM 生成更全面、深入的回答。

### 4.3 嵌入模型使用

- **維度 (Dimensions)**: `text-embedding-3-large` 支持縮短嵌入維度。為了平衡性能和成本，可以根據評估結果選擇合適的維度（如 `1536` 或 `1024`）。
- **批次處理 (Batching)**: 在索引大量文件時，應使用批次處理來調用嵌入 API，以提高效率。

## 5. 模型微調 (Future Consideration)

在專案的後期階段，當積累了足夠的高品質數據後，可以考慮對模型進行微調以提高在特定任務上的性能。

- **潛在的微調任務**:
    - **分類**: 將用戶回饋分類為 `bug`, `feature_request`, `question`。
    - **格式化**: 微調一個較小的模型，使其能可靠地將分析結果轉換為特定格式的 GitHub issue。
- **數據集**:
    - 需要創建一個高品質的 "提示-完成" (prompt-completion) 數據集。
    - 數據應來自真實的專案互動，並經過人工審查和標註。

## 6. 更新記錄

| 日期       | 版本 | 更新內容           | 更新人 |
|------------|------|--------------------|--------|
| 2025-07-30 | 1.1  | 新增智能路由與動態檢索策略說明 | Cline  |
| 2025-07-24 | 1.0  | 初始版本建立       | Cline  |
