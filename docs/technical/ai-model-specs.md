# AI Model Specifications

## 1. 概述

本文件詳細描述了 `Codex-Scribe` 專案中使用的 AI 模型（大型語言模型和嵌入模型）的選擇、配置和使用規範。

**目標讀者**: 開發者、AI 工程師。

## 2. 模型選擇

本專案採用混合模型的策略，針對不同任務選擇最合適的模型。

### 2.1 大型語言模型 (LLMs)

- **主要模型 (用於複雜推理和生成)**:
    - **`OpenAI GPT-4o`**: 作為預設的主要模型。它在程式碼理解、遵循指令和結構化輸出方面表現出色，綜合能力強。
- **備選/專用模型**:
    - **`Anthropic Claude 3.5 Sonnet`**: 作為 GPT-4o 的高性能、高性價比替代品。在需要快速響應或處理長篇程式碼的場景下可以考慮使用。
    - **`Google Gemini 1.5 Pro`**: 當需要處理超長上下文（高達 1M token）時，例如一次性分析整個大型文件或多個文件，此模型是首選。

### 2.2 嵌入模型 (Embedding Models)

- **主要模型**:
    - **`OpenAI text-embedding-3-large`**: 作為預設的嵌入模型。它性能優異，能夠很好地捕捉程式碼和自然語言的語義。
- **備選/專用模型**:
    - **`Voyage AI voyage-code-2`**: 專為程式碼優化的嵌入模型。在後續的性能評估中，如果發現程式碼語義搜索的準確性有待提高，可以考慮切換或混合使用此模型。

## 3. 模型配置與使用

### 3.1 LLM 客戶端 (`src/infrastructure/llm/llm_client.py`)

- **抽象化**: 應建立一個 `LLMClient` 的抽象基類 (ABC)，並為每個模型提供方（如 `OpenAIClient`, `AnthropicClient`）提供具體的實現。這使得在上層應用中可以輕鬆切換模型。
- **統一接口**: 所有客戶端都應提供一個統一的 `generate` 方法，接收標準化的輸入（如提示、停止序列）並返回標準化的輸出。

### 3.2 LLM 呼叫參數

- **`temperature`**:
    - 對於需要**創造性**和**生成性**的任務（如撰寫問題報告的描述），可以設置為 `0.3` - `0.5`。
    - 對於需要**確定性**和**事實性**的任務（如從文本中提取實體、格式化 JSON），應設置為 `0.0` - `0.1`。
- **`max_tokens`**: 應根據任務類型合理設置，以避免不必要的 token 浪費和成本。
- **`top_p`**: 通常保持預設值 (`1.0`)，除非需要更精細地控制輸出的隨機性。
- **JSON Mode**: 盡可能使用模型提供方支持的 JSON 模式，以確保輸出的格式正確性。

### 3.3 嵌入模型使用

- **維度 (Dimensions)**: `text-embedding-3-large` 支持縮短嵌入維度。為了平衡性能和成本，可以根據評估結果選擇合適的維度（如 `1536` 或 `1024`）。
- **批次處理 (Batching)**: 在索引大量文件時，應使用批次處理來調用嵌入 API，以提高效率。

## 4. 模型微調 (Future Consideration)

在專案的後期階段，當積累了足夠的高品質數據後，可以考慮對模型進行微調以提高在特定任務上的性能。

- **潛在的微調任務**:
    - **分類**: 將用戶回饋分類為 `bug`, `feature_request`, `question`。
    - **格式化**: 微調一個較小的模型，使其能可靠地將分析結果轉換為特定格式的 GitHub issue。
- **數據集**:
    - 需要創建一個高品質的 "提示-完成" (prompt-completion) 數據集。
    - 數據應來自真實的專案互動，並經過人工審查和標註。

## 5. 更新記錄

| 日期       | 版本 | 更新內容           | 更新人 |
|------------|------|--------------------|--------|
| 2025-07-24 | 1.0  | 初始版本建立       | Cline  |
