# Performance Benchmarks and Goals

## 1. 概述

本文件定義了 `Codex-Scribe` 專案的性能基準和優化目標。持續監控和改進性能對於提供良好的用戶體驗和控制運營成本至關重要。

**目標讀者**: 開發者、維運工程師、產品經理。

## 2. 關鍵性能指標 (KPIs)

我們將關注以下幾個核心領域的性能指標：

1.  **在線查詢性能 (Online Query Performance)**: 用戶感知的實時響應能力。
2.  **離線索引性能 (Offline Indexing Performance)**: 知識庫更新的效率。
3.  **AI 模型性能 (AI Model Performance)**: LLM 和嵌入模型的效率和成本。
4.  **系統資源使用 (System Resource Usage)**: 基礎設施的成本效益。

## 3. 性能目標

### 3.1 在線查詢性能

| 指標 | 目標 (P95) | 描述 |
| :--- | :--- | :--- |
| **端到端 API 延遲** | `< 15s` | 從接收 `/analyses` 請求到成功返回任務結果的總時間。 |
| **RAG 鏈執行時間** | `< 10s` | 從接收查詢到生成最終答案的時間（不含網絡延遲）。 |
| **向量搜索延遲** | `< 500ms` | 在 ChromaDB 中執行一次相似性搜索的時間。 |
| **圖形查詢延遲** | `< 1s` | 在 Neo4j 中執行一次典型查詢的時間。 |

### 3.2 離線索引性能

| 指標 | 目標 | 描述 |
| :--- | :--- | :--- |
| **索引吞吐量** | `> 50 files/min` | 索引管道平均每分鐘處理的文件數量。 |
| **中型專案索引時間** | `< 15 mins` | 索引一個包含約 100-200 個源碼文件的專案的總時間。 |
| **資源使用** | - | 索引任務應在合理的 CPU 和內存限制內完成。 |

### 3.3 AI 模型性能與成本

| 指標 | 目標 | 描述 |
| :--- | :--- | :--- |
| **LLM API 平均延遲** | `< 5s` | 調用 OpenAI 等服務的平均響應時間。 |
| **嵌入 API 平均延遲** | `< 1s` | 批次調用嵌入服務的平均響應時間。 |
| **每月 LLM 成本** | `< $100` (初期) | 監控並控制 LLM API 的總體花費。 |
| **Token 效率** | - | 提示的長度應盡可能簡潔，同時保證回答品質。 |

## 4. 基準測試 (Benchmarking)

### 4.1 測試環境
- **標準化**: 所有基準測試都應在一個標準化的、可重現的環境中進行（例如，特定的 AWS EC2 實例類型或本地機器的 Docker 環境）。
- **測試數據集**:
    - **程式碼庫**: 使用幾個不同大小和語言的開源專案作為標準測試集（例如，一個小型、一個中型、一個大型專案）。
    - **查詢集**: 創建一組標準化的查詢，涵蓋不同類型（語義、結構、混合）。

### 4.2 測試流程
- **自動化**: 基準測試應該被整合到一個自動化腳本中，可以定期運行。
- **版本比較**: 每次對核心算法（如文本分割、檢索策略）或模型進行重大更改後，都應運行基準測試，並將結果與前一版本進行比較。
- **結果記錄**: 所有基準測試的結果都應被記錄下來，包括日期、程式碼版本、環境配置和各項性能指標。

## 5. 性能優化策略

- **緩存 (Caching)**:
    - 對於不經常變動的數據（如已解析的 AST、已生成的嵌入），可以考慮引入緩存機制。
    - 緩存 LLM 的某些確定性查詢結果。
- **批次處理 (Batching)**: 在索引流程中，對嵌入生成和數據庫寫入操作進行批次處理。
- **異步處理**: 對於耗時的操作（如 LLM 呼叫），在 API 中採用異步處理模式。
- **模型選擇**: 根據任務的複雜度，動態選擇更小、更快的模型。
- **數據庫優化**: 為 Neo4j 中的常用查詢欄位創建索引。

## 6. 更新記錄

| 日期       | 版本 | 更新內容           | 更新人 |
|------------|------|--------------------|--------|
| 2025-07-24 | 1.0  | 初始版本建立       | Cline  |
